{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhsrj/Artificial-Intelligence-for-Dummies/blob/main/cognee_with_ontology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Welcome to cognee üß†**\n",
        "\n",
        "**cognee** is your toolkit for turning text into a structured knowledge graph, optionally enhanced by ontologies, and then querying it with advanced retrieval techniques. This notebook will guide you through a simple example.\n",
        "\n",
        "Let's start with installing cognee!\n"
      ],
      "metadata": {
        "id": "7yxqZnaSp1Bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*NOTE: Google colab will ask you to restart the session, restart it and continue with the notebook as normal afterwards.\n",
        "Ignore pip errors and warnings after restarting the session.*\n"
      ],
      "metadata": {
        "id": "mjIIDsHnzF8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Configure Environment Variables and Import cognee üõ†Ô∏è\n",
        "\n",
        "Cognee uses OpenAI's gpt-4o-mini model in the default setting. Provide your **OpenAI** API key below.\n",
        "\n",
        "*Note: OpenAI free tier does not satify the rate limit requirements.*\n",
        "\n",
        "Please refer to our documentation if you want to use another [remote model](https://docs.cognee.ai/how-to-guides/remote-models) or a [local model](https://docs.cognee.ai/how-to-guides/local-models)."
      ],
      "metadata": {
        "id": "mWg-D5qerGot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Upload Sample Data from cognee repo\n",
        "\n",
        "We'll upload a text file from our repo containing two text variables (`text_1` and `text_2`) which are brief introductions to German car manufacturers and major tech companies.\n"
      ],
      "metadata": {
        "id": "-VNiDTYc4nfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O car_and_tech_companies.txt https://raw.githubusercontent.com/topoteretes/cognee/dev/examples/data/car_and_tech_companies.txt\n",
        "input_text = \"/content/car_and_tech_companies.txt\"\n",
        "\n",
        "# uncomment the print statement below to view the file content\n",
        "# print(open(input_text, 'r').read())"
      ],
      "metadata": {
        "id": "fQNflDJMuDVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ce0409-2252-47c7-84da-2e1cd4853a3d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-06 19:24:34--  https://raw.githubusercontent.com/topoteretes/cognee/dev/examples/data/car_and_tech_companies.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4321 (4.2K) [text/plain]\n",
            "Saving to: ‚Äòcar_and_tech_companies.txt‚Äô\n",
            "\n",
            "\r          car_and_t   0%[                    ]       0  --.-KB/s               \rcar_and_tech_compan 100%[===================>]   4.22K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-09-06 19:24:34 (29.4 MB/s) - ‚Äòcar_and_tech_companies.txt‚Äô saved [4321/4321]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario 1: cognee With Ontology\n",
        "\n",
        "Think of an ontology as a map or blueprint for organizing information: it defines ‚Äúwhat types of things exist‚Äù (e.g. CarManufacturer, CarModel) and the relationships among them (produces, belongsTo). By mapping raw text to this ontology, cognee can create a more structured knowledge graph.\n"
      ],
      "metadata": {
        "id": "xFesSgm-m_7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import litellm"
      ],
      "metadata": {
        "id": "lwg1aLHEjVBb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "litellm._turn_on_debug()"
      ],
      "metadata": {
        "id": "L8SzNc9ojOHZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastembed"
      ],
      "metadata": {
        "id": "Dhh_UYRalReJ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cognee\n",
        "import os\n",
        "# Import the specific configuration getter functions\n",
        "from cognee.infrastructure.llm.config import get_llm_config\n",
        "from cognee.infrastructure.databases.vector import get_vectordb_config\n"
      ],
      "metadata": {
        "id": "-kjNQRM5lWn4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cognee\n",
        "\n",
        "# --- 1. LLM Configuration (Free local provider with Ollama) ---\n",
        "llm_settings = {\n",
        "    \"llm_provider\": \"ollama\",  # switch to Ollama\n",
        "    \"llm_model\": \"llama2:7b\",  # example local model\n",
        "    # Ollama runs locally; typically no API key needed\n",
        "    # You may set endpoint if applicable, else omit or use default\n",
        "}\n",
        "\n",
        "cognee.config.set_llm_config(llm_settings)\n",
        "\n"
      ],
      "metadata": {
        "id": "NKQeIonJlcu5"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 2. Embedding Model Configuration (Local/Free Provider) ---\n",
        "# Add this section to override the OpenAI embedding default.\n",
        "# We'll configure cognee to use a free, local model via fastembed.\n",
        "\n",
        "# Create a dictionary for vector/embedding settings.\n",
        "# The keys \"embedding_provider\" and \"embedding_model\" are standard.\n",
        "embedding_settings_dictionary = {\n",
        "    \"embedding_provider\": \"fastembed\",\n",
        "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\", # Popular local model\n",
        "    # \"vector_db_key\": None # Explicitly set key to None if a setter requires it\n",
        "}\n",
        "\n",
        "# Use the set_vector_db_config method from config.py to apply settings.\n",
        "# This assumes embedding settings are part of the vector DB configuration.\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"--- LLM Config ---\")\n",
        "llm_conf = get_llm_config()\n",
        "print(f\"Provider: {llm_conf.llm_provider}\")\n",
        "print(f\"Model: {llm_conf.llm_model}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF9s8l9Zlevp",
        "outputId": "cb6c0aaa-7cdd-4f3a-8506-84cb60be28ee"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- LLM Config ---\n",
            "Provider: ollama\n",
            "Model: llama2:7b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First we'll clean any old data and resets system metadata so we start from a blank slate.\n",
        "await cognee.prune.prune_data()\n",
        "await cognee.prune.prune_system(metadata=True)\n",
        "\n",
        "# Next, we add the input text to cognee‚Äôs data store.\n",
        "await cognee.add(input_text)\n",
        "\n",
        "# Now we'll upload the ontology file from cognee repo. This provides a structure for various types of companies (car manufacturers, tech companies), products they make, and the categories of those products.\n",
        "!wget -O basic_ontology.owl https://raw.githubusercontent.com/topoteretes/cognee/main/examples/python/ontology_input_example/basic_ontology.owl\n",
        "ontology_path = \"/content/basic_ontology.owl\"\n",
        "\n",
        "# We'll give the ontology file as a parameter to cognify, cognee's main pipeline. Cognify is the process that transforms the raw text into a knowledge graph.\n",
        "await cognee.cognify(ontology_file_path=ontology_path)"
      ],
      "metadata": {
        "id": "tM_XHyT2yDYZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "outputId": "6fd1c669-fd42-41e2-b876-0b4a7051dda2"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[2m2025-09-06T20:04:56.282095\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mFile /usr/local/lib/python3.12/dist-packages/cognee/.cognee_system/databases/cognee_graph.pkl not found. Initializing an empty graph.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
            "\u001b[2m2025-09-06T20:04:56.286027\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mGraph deleted successfully.   \u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
            "\u001b[2m2025-09-06T20:04:56.292299\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDatabase deleted successfully.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
            "\u001b[2m2025-09-06T20:04:56.667589\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel not found in LiteLLM's model_cost.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
            "\u001b[1mHTTP Request: POST https://generativelanguage.googleapis.com/chat/completions \"HTTP/1.1 404 Not Found\"\u001b[0m\n",
            "\u001b[1mHTTP Request: POST https://generativelanguage.googleapis.com/chat/completions \"HTTP/1.1 404 Not Found\"\u001b[0m\n",
            "\u001b[1mHTTP Request: POST https://generativelanguage.googleapis.com/chat/completions \"HTTP/1.1 404 Not Found\"\u001b[0m\n",
            "\u001b[1mHTTP Request: POST https://generativelanguage.googleapis.com/chat/completions \"HTTP/1.1 404 Not Found\"\u001b[0m\n",
            "\u001b[1mHTTP Request: POST https://generativelanguage.googleapis.com/chat/completions \"HTTP/1.1 404 Not Found\"\u001b[0m\n",
            "\u001b[2m2025-09-06T20:04:57.476228\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError code: 404               \u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
            "\u001b[2m2025-09-06T20:04:57.477727\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mConnection to LLM could not be established.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "InstructorRetryException",
          "evalue": "Error code: 404",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/retry.py\u001b[0m in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, args, kwargs, context, max_retries, strict, mode, hooks)\u001b[0m\n\u001b[1;32m    250\u001b[0m                             \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit_completion_last_attempt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/retry.py\u001b[0m in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, args, kwargs, context, max_retries, strict, mode, hooks)\u001b[0m\n\u001b[1;32m    184\u001b[0m                     \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit_completion_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit_completion_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/retry.py\u001b[0m in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, args, kwargs, context, max_retries, strict, mode, hooks)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmax_retries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    420\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRetryError\u001b[0m: RetryError[<Future at 0x78bd10aba4b0 state=finished raised NotFoundError>]",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mInstructorRetryException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1595994556.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Next, we add the input text to cognee‚Äôs data store.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mcognee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Now we'll upload the ontology file from cognee repo. This provides a structure for various types of companies (car manufacturers, tech companies), products they make, and the categories of those products.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cognee/api/v1/add/add.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(data, dataset_name, user, node_set)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolve_data_directories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mingest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     await cognee_pipeline(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"add_pipeline\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cognee/modules/pipelines/operations/pipeline.py\u001b[0m in \u001b[0;36mcognee_pipeline\u001b[0;34m(tasks, data, datasets, user, pipeline_name)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Test LLM and Embedding configuration once before running Cognee\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0mtest_llm_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0mtest_embedding_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mcognee_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# Update flag after first run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cognee/infrastructure/llm/utils.py\u001b[0m in \u001b[0;36mtest_llm_connection\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Connection to LLM could not be established.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cognee/infrastructure/llm/utils.py\u001b[0m in \u001b[0;36mtest_llm_connection\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mllm_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_llm_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         await llm_adapter.acreate_structured_output(\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mtext_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0msystem_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Respond to me with the following string: \"test\"'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cognee/infrastructure/llm/rate_limiter.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                     \u001b[0mattempt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cognee/infrastructure/llm/rate_limiter.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwaited\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Rate limited LLM API call, waited for {waited}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cognee/infrastructure/llm/ollama/adapter.py\u001b[0m in \u001b[0;36macreate_structured_output\u001b[0;34m(self, text_input, system_prompt, response_model)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         response = self.aclient.chat.completions.create(\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/client.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, response_model, messages, max_retries, validation_context, context, strict, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         return self.create_fn(\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0mresponse_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/patch.py\u001b[0m in \u001b[0;36mnew_create_sync\u001b[0;34m(response_model, validation_context, context, max_retries, strict, hooks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m  \u001b[0;31m# type: ignore[return-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         response = retry_sync(\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mresponse_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/instructor/core/retry.py\u001b[0m in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, args, kwargs, context, max_retries, strict, mode, hooks)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Retry error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         raise InstructorRetryException(\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mlast_completion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInstructorRetryException\u001b[0m: Error code: 404"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's visualize the knowledge graph üëÄ\n",
        "\n",
        "Below we'll let cognee render an HTML file for graph visualization. The file will be stored in the artifacts folder and and automatically downloaded.\n",
        "\n",
        "Please open the downloaded file in your browser to view the graph.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EZT2Fh_t_OF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "from google.colab import files\n",
        "\n",
        "notebook_dir = pathlib.Path.cwd()\n",
        "graph_file_path = (notebook_dir / \"artifacts\" / \"graph_visualization_with_ontology.html\").resolve()\n",
        "\n",
        "await visualize_graph(str(graph_file_path))\n",
        "\n",
        "files.download('./artifacts/graph_visualization_with_ontology.html')"
      ],
      "metadata": {
        "id": "DlDQQ8ZdXf9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can now ask cognee about the data that we cognify'ed.\n",
        "\n",
        "We'll use `GRAPH_COMPLETION` as our search type. It retrieves most related entities from the knowledge graph to user query and prompts LLM to answer with it.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P9PDoYHP9AoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_results_with_ontology = await cognee.search(\n",
        "    query_type=SearchType.GRAPH_COMPLETION,\n",
        "    query_text=\"What are the exact cars and their types produced by Audi?\",\n",
        ")\n",
        "print(search_results_with_ontology)"
      ],
      "metadata": {
        "id": "6pvd7pqCVgjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario 2: Base cognee\n",
        "\n",
        "What if you don‚Äôt have an ontology? Cognee can still parse and connect entities out of the box.\n",
        "\n",
        "Now we'll add the same text input to cognee without ontology to see the difference in the graph and the search result."
      ],
      "metadata": {
        "id": "bQXto5yanVmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clean up the cognee store again for a fresh start.\n",
        "await cognee.prune.prune_data()\n",
        "await cognee.prune.prune_system(metadata=True)\n",
        "\n",
        "# add text input to cognee\n",
        "await cognee.add(input_text)\n",
        "\n",
        "# cognify!\n",
        "await cognee.cognify()"
      ],
      "metadata": {
        "id": "l4so4wBknUaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's look at the knowledge graph again, this time without ontology"
      ],
      "metadata": {
        "id": "qRhdyiLVLMaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "from google.colab import files\n",
        "\n",
        "notebook_dir = pathlib.Path.cwd()\n",
        "graph_file_path = (notebook_dir / \"artifacts\" / \"graph_visualization_base_cognee.html\").resolve()\n",
        "\n",
        "await visualize_graph(str(graph_file_path))\n",
        "\n",
        "files.download('./artifacts/graph_visualization_base_cognee.html')"
      ],
      "metadata": {
        "id": "ip2w_AaRoCk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### And we'll ask the same question about the car models from Audi ‚¨áÔ∏è"
      ],
      "metadata": {
        "id": "VVpyRWcwLqRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_results_base_cognee = await cognee.search(\n",
        "    query_type=SearchType.GRAPH_COMPLETION,\n",
        "    query_text=\"What are the exact cars and their types produced by Audi?\",\n",
        ")\n",
        "print(search_results_base_cognee)"
      ],
      "metadata": {
        "id": "W8Zuir67n9eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario 3: Traditional vector-based RAG\n",
        "\n",
        "RAG (Retrieval-Augmented Generation) is another approach that uses vector embeddings to find relevant text chunks, then generates an answer using a language model.\n",
        "\n",
        "Search type `RAG_COMPLETION` follows this logic, get's a document chunk most related to the user query and prompts the LLM with it.\n",
        "\n",
        "This differs from `GRAPH_COMPLETION`, which relies on explicit relationships stored in the knowledge graph.\n"
      ],
      "metadata": {
        "id": "dYiGONQapW-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_results_traditional_rag = await cognee.search(\n",
        "    query_type=SearchType.RAG_COMPLETION,\n",
        "    query_text=\"What are the exact cars and their types produced by Audi?\",\n",
        ")\n",
        "print(search_results_traditional_rag)"
      ],
      "metadata": {
        "id": "8vlo09GooJFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's compare all the results!\n",
        "\n",
        "Notice how the ontology approach can yield more structured result.\n"
      ],
      "metadata": {
        "id": "woExlCJ5R48c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(search_results_with_ontology)"
      ],
      "metadata": {
        "id": "hc5DjPTJtJPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(search_results_base_cognee)"
      ],
      "metadata": {
        "id": "LRlT1vyZtDqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(search_results_traditional_rag)"
      ],
      "metadata": {
        "id": "uoxiubGdqG_V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}